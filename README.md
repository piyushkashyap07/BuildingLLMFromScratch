<!-- Building LLm From Scratch -->
## BUILD LLM FROM SCRATCH

This repository documents my journey in building a Large Language Model (LLM) from scratch. The project covers everything from preprocessing to model training and fine-tuning.

### 🚀 Current Progress: Preprocessing  
I am currently working on the preprocessing phase, which includes:  
- **Tokenizer:** Custom implementation for tokenizing text data (**`Tokenizer_Part-1.ipynb`**).  
- **Data Creation:** Preparing and structuring data for training (**`data_creation.ipynb`**).

### 📌 Upcoming Steps  
- Model architecture design  
- Training the LLM  
- Fine-tuning and evaluation  

### 📚 Prerequisites  
To follow along, make sure you have:  
- Python 3.9
- `numpy`, `torch`, `transformers` (if applicable)  
- Any other dependencies will be added as the project progresses  

### 📂 Structure (Work in Progress)  
```
BUILD-LLM-FROM-SCRATCH/
│── preprocessing/
│   ├── Tokenizer_Part-1.ipynb
│   ├── data_creation.ipynb
│── README.md
```

### ⭐ Stay Tuned  
I’ll be updating this repository as I progress. Contributions and discussions are welcome!  
