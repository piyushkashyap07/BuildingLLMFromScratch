<!-- Building LLm From Scratch -->
## BUILD LLM FROM SCRATCH

This repository documents my journey in building a Large Language Model (LLM) from scratch. The project covers everything from preprocessing to model training and fine-tuning.

### ğŸš€ Current Progress: Preprocessing  
I am currently working on the preprocessing phase, which includes:  
- **Tokenizer:** Custom implementation for tokenizing text data (**`Tokenizer_Part-1.ipynb`**).  
- **Data Creation:** Preparing and structuring data for training (**`data_creation.ipynb`**).

### ğŸ“Œ Upcoming Steps  
- Model architecture design  
- Training the LLM  
- Fine-tuning and evaluation  

### ğŸ“š Prerequisites  
To follow along, make sure you have:  
- Python 3.9
- `numpy`, `torch`, `transformers` (if applicable)  
- Any other dependencies will be added as the project progresses  

### ğŸ“‚ Structure (Work in Progress)  
```
BUILD-LLM-FROM-SCRATCH/
â”‚â”€â”€ preprocessing/
â”‚   â”œâ”€â”€ Tokenizer_Part-1.ipynb
â”‚   â”œâ”€â”€ data_creation.ipynb
â”‚â”€â”€ README.md
```

### â­ Stay Tuned  
Iâ€™ll be updating this repository as I progress. Contributions and discussions are welcome!  
